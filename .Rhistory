res <- curlPerform(postfields = formdata, url = base.url, curl = curl,
post = 1L, writefunction = r$update)
result.string <- r$value()
# Parse the page to get freeway data and write to a CSV file.
freeways <- getFreeways(result.string)
write.csv(freeways, paste(data.folder, "freeways.csv", sep="/"), row.names=F)
# Select only those freeways which are of interest to us.
freeways <- subsetFreeways(freeways, freeways.of.interest.file)
# Since we are intersted in a specific postmile range
freeways <- indicatePostmiles(freeways)
build_spatial_multistation_df(freeways, start, end, c("flow","occ","speed"), base.url = base.url)
# tic()
# example_df <- df_build()
# toc()
# write.csv(example_df, "/Users/sssantos/Documents/STA160/160trafficdata/data/example_df")
############################################################################################
# Functions for getting Data at Freeway Level
############################################################################################
getSpatial <- function(freeway, direction, abspm_start, abspm_end, quantity = 'flow',
start_search.date.str = start, end_search.date.str = end,
curl, base.url = base.url, granularity = 'hour', data.folder) {
s.time.id <- as.character(as.integer(as.POSIXct(start_search.date.str,
origin="1970-01-01",
tz = "GMT")))
e.time.id <- as.character(as.integer(as.POSIXct(end_search.date.str,
origin="1970-01-01",
tz = "GMT")))
# Parse the search.date.str into a vector
start_search.date.v <- unlist(strsplit(start_search.date.str, '-'))
names(start_search.date.v) <- c("year", "month", "day")
# Combine variables into a "start date" (sdate) string
sdate <- paste(paste(start_search.date.v[['month']],
start_search.date.v[['day']],
start_search.date.v[['year']],
sep='%2F'),
'+00%3A00', sep = '')
# Parse the search.date.str into a vector
end_search.date.v <- unlist(strsplit(end_search.date.str, '-'))
names(end_search.date.v) <- c("year", "month", "day")
# Combine variables into a "start date" (sdate) string
edate <- paste(paste(end_search.date.v[['month']],
end_search.date.v[['day']],
end_search.date.v[['year']],
sep='%2F'),
'+00%3A00', sep = '')
# Combine variables into a "file date" (fdate) string
fdate <- paste(start_search.date.v[['year']],
start_search.date.v[['month']],
start_search.date.v[['day']],
sep='')
# Page configuration - query specification for type of report page
form.num <- '1'
node.name <- 'Freeway'
content <- '&content=spatial&tab=mst'
export.type <- 'xls'
# Combine variables into a "page" (page) string
page <- paste('report_form=', form.num, '&dnode=', node.name, '&content=',
content, '&export=', export.type, sep='')
r.url <- paste(base.url, '/?', page, '&fwy=', freeway, '&dir=', direction,
'&s_time_id=', s.time.id, '&s_time_id_f=', sdate,
'&e_time_id=', e.time.id, '&e_time_id_f=', edate,
'&q=', quantity, '&q2=&agg=on',
'&start_pm=', abspm_start, '&end_pm=', abspm_end,
'&gn=', granularity, '&ihv=on&html.x=50&html.y=12',
sep='')
output.filename <- paste(data.folder, '/', node.name, '_',
quantity, '_', freeway, direction, '_', fdate,
'.xls', sep='')
# Get TSV data file from website and store as a string in memory
if(!file.exists(output.filename)) {
r = dynCurlReader()
z <- getURLContent(url = r.url, curl = curl, binary = TRUE)
con = file(output.filename, "wb")
.Internal(writeBin(z, con, 1, FALSE, TRUE))
close(con)
}
freeway_data <- suppressWarnings(read.xlsx2(output.filename,1))
#Formatting data frame
fd <- sapply(freeway_data, as.character)
x <- ncol(fd)
c <- fd[,-c(1,x-1,x)]
c <- sapply(data.frame(c, stringsAsFactors = FALSE), as.numeric)
avg_col <- round((rowSums(c)) / (x-3))
df <- data.frame(fd[,1],avg_col,as.numeric(fd[,x-1]), as.numeric(fd[,x]))
frwy_col <- rep(freeway,   nrow(df))
dir_col  <- rep(direction, nrow(df))
df <- add_column(df, dir_col, .after = 1 )
df <- add_column(df, frwy_col,.after = 1 )
averaged_value <- paste("Average ", quantity, sep = "")
names(df) <- c("TimeStamp", "Freeway", "Direction", averaged_value,"Lane Points","Percent Observed")
df
}
getSpatialMulti <- function(freeway, direction, abspm_start, abspm_end, quantities = c('flow','occ','speed'),
start_search.date.str = start, end_search.date.str = end,
curl, base.url, granularity = 'hour', data.folder) {
Reduce(merge,
lapply(quantities, function(x) {
getSpatial(freeway, direction, quantity = x,
start_search.date.str = start, end_search.date.str = end,
curl, base.url = base.url, granularity = 'hour', data.folder)
})
)
}
build_spatial_multistation_df <- function(freeways_df, start, end, quantities, base.url) {rbindlist(lapply(data.frame(t(freeways)), function(x) {
name  <- x[[2]]
dir   <- x[[3]]
s_pm  <- x[[4]]
e_pm  <- x[[5]]
getSpatialMulti(name,dir, abspm_start = s_pm, abspm_end = e_pm, quantities, start_search.date.str = start, end_search.date.str = end,
curl = curl, base.url = base.url, data.folder = data.folder)
}))
}
# - Freeway-lane entries must be listed as one entry per line
# - Entries much match this "regex": ^(?:I|SR|US)\\d+[NSEW]?-[NSEW]{1}$
# - Where ^(?:I|SR|US) means: starts with I or SR or US
# - And \\d+[NSEW]?- means:
#   - one or more digits
#   - *optionally* followed by a single N or S or E or W
#   - followed by a single dash
# - And [NSEW]{1}$ means: ends with a single N or S or E or W
# - Example: SR24-W
# - Example: I880S-S
freeways.of.interest.file <- "freeways_of_interest.txt"
build_spatial_multistation_df(freeways, start, end, c("flow","occ","speed"), base.url = base.url)
args(getSpatial)
getSpatial("880")
# tic()
# example_df <- df_build()
# toc()
# write.csv(example_df, "/Users/sssantos/Documents/STA160/160trafficdata/data/example_df")
############################################################################################
# Functions for getting Data at Freeway Level
############################################################################################
getSpatial <- function(freeway, direction, abspm_start, abspm_end, quantity = 'flow',
start_search.date.str = start, end_search.date.str = end,
curl, base.url, granularity = 'hour', data.folder) {
s.time.id <- as.character(as.integer(as.POSIXct(start_search.date.str,
origin="1970-01-01",
tz = "GMT")))
e.time.id <- as.character(as.integer(as.POSIXct(end_search.date.str,
origin="1970-01-01",
tz = "GMT")))
# Parse the search.date.str into a vector
start_search.date.v <- unlist(strsplit(start_search.date.str, '-'))
names(start_search.date.v) <- c("year", "month", "day")
# Combine variables into a "start date" (sdate) string
sdate <- paste(paste(start_search.date.v[['month']],
start_search.date.v[['day']],
start_search.date.v[['year']],
sep='%2F'),
'+00%3A00', sep = '')
# Parse the search.date.str into a vector
end_search.date.v <- unlist(strsplit(end_search.date.str, '-'))
names(end_search.date.v) <- c("year", "month", "day")
# Combine variables into a "start date" (sdate) string
edate <- paste(paste(end_search.date.v[['month']],
end_search.date.v[['day']],
end_search.date.v[['year']],
sep='%2F'),
'+00%3A00', sep = '')
# Combine variables into a "file date" (fdate) string
fdate <- paste(start_search.date.v[['year']],
start_search.date.v[['month']],
start_search.date.v[['day']],
sep='')
# Page configuration - query specification for type of report page
form.num <- '1'
node.name <- 'Freeway'
content <- '&content=spatial&tab=mst'
export.type <- 'xls'
# Combine variables into a "page" (page) string
page <- paste('report_form=', form.num, '&dnode=', node.name, '&content=',
content, '&export=', export.type, sep='')
r.url <- paste(base.url, '/?', page, '&fwy=', freeway, '&dir=', direction,
'&s_time_id=', s.time.id, '&s_time_id_f=', sdate,
'&e_time_id=', e.time.id, '&e_time_id_f=', edate,
'&q=', quantity, '&q2=&agg=on',
'&start_pm=', abspm_start, '&end_pm=', abspm_end,
'&gn=', granularity, '&ihv=on&html.x=50&html.y=12',
sep='')
output.filename <- paste(data.folder, '/', node.name, '_',
quantity, '_', freeway, direction, '_', fdate,
'.xls', sep='')
# Get TSV data file from website and store as a string in memory
if(!file.exists(output.filename)) {
r = dynCurlReader()
z <- getURLContent(url = r.url, curl = curl, binary = TRUE)
con = file(output.filename, "wb")
.Internal(writeBin(z, con, 1, FALSE, TRUE))
close(con)
}
freeway_data <- suppressWarnings(read.xlsx2(output.filename,1))
#Formatting data frame
fd <- sapply(freeway_data, as.character)
x <- ncol(fd)
c <- fd[,-c(1,x-1,x)]
c <- sapply(data.frame(c, stringsAsFactors = FALSE), as.numeric)
avg_col <- round((rowSums(c)) / (x-3))
df <- data.frame(fd[,1],avg_col,as.numeric(fd[,x-1]), as.numeric(fd[,x]))
frwy_col <- rep(freeway,   nrow(df))
dir_col  <- rep(direction, nrow(df))
df <- add_column(df, dir_col, .after = 1 )
df <- add_column(df, frwy_col,.after = 1 )
averaged_value <- paste("Average ", quantity, sep = "")
names(df) <- c("TimeStamp", "Freeway", "Direction", averaged_value,"Lane Points","Percent Observed")
df
}
getSpatialMulti <- function(freeway, direction, abspm_start, abspm_end, quantities = c('flow','occ','speed'),
start_search.date.str = start, end_search.date.str = end,
curl, base.url, granularity = 'hour', data.folder) {
Reduce(merge,
lapply(quantities, function(x) {
getSpatial(freeway, direction, quantity = x,
start_search.date.str = start, end_search.date.str = end,
curl, base.url = base.url, granularity = 'hour', data.folder)
})
)
}
build_spatial_multistation_df <- function(freeways_df, start, end, quantities, base.url) {rbindlist(lapply(data.frame(t(freeways)), function(x) {
name  <- x[[2]]
dir   <- x[[3]]
s_pm  <- x[[4]]
e_pm  <- x[[5]]
getSpatialMulti(name,dir, abspm_start = s_pm, abspm_end = e_pm, quantities, start_search.date.str = start, end_search.date.str = end,
curl = curl, base.url = base.url, data.folder = data.folder)
}))
}
args(getSpatial)
getSpatial("880", "S", '0','33', 'flow',start,end,curl,base.url,"hour",data.folder)
getSpatialMulti(("880", "S", '0','33', c('flow','occ'),start,end,curl,base.url,"hour",data.folder)
getSpatialMulti("880", "S", '0','33', c('flow','occ'),start,end,curl,base.url,"hour",data.folder)
getSpatialMulti <- function(freeway, direction, abspm_start, abspm_end, quantities = c('flow','occ','speed'),
start_search.date.str = start, end_search.date.str = end,
curl, base.url, granularity = 'hour', data.folder) {
Reduce(merge,
lapply(quantities, function(x) {
getSpatial(freeway, direction, quantity = x,
start_search.date.str = start, end_search.date.str = end,
curl, base.url, granularity = 'hour', data.folder)
})
)
}
getSpatialMulti("880", "S", '0','33', c('flow','occ'),start,end,curl,base.url,"hour",data.folder)
getSpatialMulti("880", "S", '0','33', c('flow','occ'),start,end,curl,base.url = base.url,"hour",data.folder)
getSpatialMulti <- function(freeway, direction, abspm_start, abspm_end, quantities = c('flow','occ','speed'),
start_search.date.str = start, end_search.date.str = end,
curl, base.url, granularity = 'hour', data.folder) {
burl = base.url
Reduce(merge,
lapply(quantities, function(x) {
getSpatial(freeway, direction, quantity = x,
start_search.date.str = start, end_search.date.str = end,
curl, burl, granularity = 'hour', data.folder)
})
)
}
getSpatialMulti("880", "S", '0','33', c('flow','occ'),start,end,curl,base.url = base.url,"hour",data.folder)
getSpatialMulti <- function(freeway, direction, abspm_start, abspm_end, quantities = c('flow','occ','speed'),
start_search.date.str = start, end_search.date.str = end,
curl, base.url, granularity = 'hour', data.folder) {
Reduce(merge,
lapply(quantities, function(x) {
burl = base.url
getSpatial(freeway, direction, quantity = x,
start_search.date.str = start, end_search.date.str = end,
curl, base.url = burl, granularity = 'hour', data.folder)
})
)
}
getSpatialMulti("880", "S", '0','33', c('flow','occ'),start,end,curl,base.url = base.url,"hour",data.folder)
getSpatialMulti("880", "S", '0','33', c('flow','occ'),start,end,curl,base.url,"hour",data.folder)
getSpatial("880", "S", '0','33', 'flow',start,end,curl,base.url,"hour",data.folder)
# tic()
# example_df <- df_build()
# toc()
# write.csv(example_df, "/Users/sssantos/Documents/STA160/160trafficdata/data/example_df")
############################################################################################
# Functions for getting Data at Freeway Level
############################################################################################
getSpatial <- function(freeway, direction, abspm_start, abspm_end, quantity = 'flow',
start_search.date.str = start, end_search.date.str = end,
curl, base.url, granularity = 'hour', data.folder) {
s.time.id <- as.character(as.integer(as.POSIXct(start_search.date.str,
origin="1970-01-01",
tz = "GMT")))
e.time.id <- as.character(as.integer(as.POSIXct(end_search.date.str,
origin="1970-01-01",
tz = "GMT")))
# Parse the search.date.str into a vector
start_search.date.v <- unlist(strsplit(start_search.date.str, '-'))
names(start_search.date.v) <- c("year", "month", "day")
# Combine variables into a "start date" (sdate) string
sdate <- paste(paste(start_search.date.v[['month']],
start_search.date.v[['day']],
start_search.date.v[['year']],
sep='%2F'),
'+00%3A00', sep = '')
# Parse the search.date.str into a vector
end_search.date.v <- unlist(strsplit(end_search.date.str, '-'))
names(end_search.date.v) <- c("year", "month", "day")
# Combine variables into a "start date" (sdate) string
edate <- paste(paste(end_search.date.v[['month']],
end_search.date.v[['day']],
end_search.date.v[['year']],
sep='%2F'),
'+00%3A00', sep = '')
# Combine variables into a "file date" (fdate) string
fdate <- paste(start_search.date.v[['year']],
start_search.date.v[['month']],
start_search.date.v[['day']],
sep='')
# Page configuration - query specification for type of report page
form.num <- '1'
node.name <- 'Freeway'
content <- '&content=spatial&tab=mst'
export.type <- 'xls'
# Combine variables into a "page" (page) string
page <- paste('report_form=', form.num, '&dnode=', node.name, '&content=',
content, '&export=', export.type, sep='')
r.url <- paste(base.url, '/?', page, '&fwy=', freeway, '&dir=', direction,
'&s_time_id=', s.time.id, '&s_time_id_f=', sdate,
'&e_time_id=', e.time.id, '&e_time_id_f=', edate,
'&q=', quantity, '&q2=&agg=on',
'&start_pm=', abspm_start, '&end_pm=', abspm_end,
'&gn=', granularity, '&ihv=on&html.x=50&html.y=12',
sep='')
output.filename <- paste(data.folder, '/', node.name, '_',
quantity, '_', freeway, direction, '_', fdate,
'.xls', sep='')
# Get TSV data file from website and store as a string in memory
if(!file.exists(output.filename)) {
r = dynCurlReader()
z <- getURLContent(url = r.url, curl = curl, binary = TRUE)
con = file(output.filename, "wb")
.Internal(writeBin(z, con, 1, FALSE, TRUE))
close(con)
}
freeway_data <- suppressWarnings(read.xlsx2(output.filename,1))
#Formatting data frame
fd <- sapply(freeway_data, as.character)
x <- ncol(fd)
c <- fd[,-c(1,x-1,x)]
c <- sapply(data.frame(c, stringsAsFactors = FALSE), as.numeric)
avg_col <- round((rowSums(c)) / (x-3))
df <- data.frame(fd[,1],avg_col,as.numeric(fd[,x-1]), as.numeric(fd[,x]))
frwy_col <- rep(freeway,   nrow(df))
dir_col  <- rep(direction, nrow(df))
df <- add_column(df, dir_col, .after = 1 )
df <- add_column(df, frwy_col,.after = 1 )
averaged_value <- paste("Average ", quantity, sep = "")
names(df) <- c("TimeStamp", "Freeway", "Direction", averaged_value,"Lane Points","Percent Observed")
df
}
getSpatialMulti <- function(freeway, direction, abspm_start, abspm_end, quantities = c('flow','occ','speed'),
start_search.date.str = start, end_search.date.str = end,
curl, base.url, granularity = 'hour', data.folder) {
Reduce(merge,
lapply(quantities, function(x) {
burl = base.url
getSpatial(freeway, direction, quantity = x, abspm_start = abspm_start, abspm_end = abspm_end
start_search.date.str = start, end_search.date.str = end,
curl, base.url, granularity = 'hour', data.folder)
})
)
}
# tic()
# example_df <- df_build()
# toc()
# write.csv(example_df, "/Users/sssantos/Documents/STA160/160trafficdata/data/example_df")
############################################################################################
# Functions for getting Data at Freeway Level
############################################################################################
getSpatial <- function(freeway, direction, abspm_start, abspm_end, quantity = 'flow',
start_search.date.str = start, end_search.date.str = end,
curl, base.url, granularity = 'hour', data.folder) {
s.time.id <- as.character(as.integer(as.POSIXct(start_search.date.str,
origin="1970-01-01",
tz = "GMT")))
e.time.id <- as.character(as.integer(as.POSIXct(end_search.date.str,
origin="1970-01-01",
tz = "GMT")))
# Parse the search.date.str into a vector
start_search.date.v <- unlist(strsplit(start_search.date.str, '-'))
names(start_search.date.v) <- c("year", "month", "day")
# Combine variables into a "start date" (sdate) string
sdate <- paste(paste(start_search.date.v[['month']],
start_search.date.v[['day']],
start_search.date.v[['year']],
sep='%2F'),
'+00%3A00', sep = '')
# Parse the search.date.str into a vector
end_search.date.v <- unlist(strsplit(end_search.date.str, '-'))
names(end_search.date.v) <- c("year", "month", "day")
# Combine variables into a "start date" (sdate) string
edate <- paste(paste(end_search.date.v[['month']],
end_search.date.v[['day']],
end_search.date.v[['year']],
sep='%2F'),
'+00%3A00', sep = '')
# Combine variables into a "file date" (fdate) string
fdate <- paste(start_search.date.v[['year']],
start_search.date.v[['month']],
start_search.date.v[['day']],
sep='')
# Page configuration - query specification for type of report page
form.num <- '1'
node.name <- 'Freeway'
content <- '&content=spatial&tab=mst'
export.type <- 'xls'
# Combine variables into a "page" (page) string
page <- paste('report_form=', form.num, '&dnode=', node.name, '&content=',
content, '&export=', export.type, sep='')
r.url <- paste(base.url, '/?', page, '&fwy=', freeway, '&dir=', direction,
'&s_time_id=', s.time.id, '&s_time_id_f=', sdate,
'&e_time_id=', e.time.id, '&e_time_id_f=', edate,
'&q=', quantity, '&q2=&agg=on',
'&start_pm=', abspm_start, '&end_pm=', abspm_end,
'&gn=', granularity, '&ihv=on&html.x=50&html.y=12',
sep='')
output.filename <- paste(data.folder, '/', node.name, '_',
quantity, '_', freeway, direction, '_', fdate,
'.xls', sep='')
# Get TSV data file from website and store as a string in memory
if(!file.exists(output.filename)) {
r = dynCurlReader()
z <- getURLContent(url = r.url, curl = curl, binary = TRUE)
con = file(output.filename, "wb")
.Internal(writeBin(z, con, 1, FALSE, TRUE))
close(con)
}
freeway_data <- suppressWarnings(read.xlsx2(output.filename,1))
#Formatting data frame
fd <- sapply(freeway_data, as.character)
x <- ncol(fd)
c <- fd[,-c(1,x-1,x)]
c <- sapply(data.frame(c, stringsAsFactors = FALSE), as.numeric)
avg_col <- round((rowSums(c)) / (x-3))
df <- data.frame(fd[,1],avg_col,as.numeric(fd[,x-1]), as.numeric(fd[,x]))
frwy_col <- rep(freeway,   nrow(df))
dir_col  <- rep(direction, nrow(df))
df <- add_column(df, dir_col, .after = 1 )
df <- add_column(df, frwy_col,.after = 1 )
averaged_value <- paste("Average ", quantity, sep = "")
names(df) <- c("TimeStamp", "Freeway", "Direction", averaged_value,"Lane Points","Percent Observed")
df
}
getSpatialMulti <- function(freeway, direction, abspm_start, abspm_end, quantities = c('flow','occ','speed'),
start_search.date.str = start, end_search.date.str = end,
curl, base.url, granularity = 'hour', data.folder) {
Reduce(merge,
lapply(quantities, function(x) {
burl = base.url
getSpatial(freeway, direction, quantity = x, abspm_start = abspm_start, abspm_end = abspm_end
start_search.date.str = start, end_search.date.str = end,
curl, base.url, granularity = 'hour', data.folder)
})
)
}
getSpatialMulti <- function(freeway, direction, abspm_start, abspm_end, quantities = c('flow','occ','speed'),
start_search.date.str = start, end_search.date.str = end,
curl, base.url, granularity = 'hour', data.folder) {
Reduce(merge,
lapply(quantities, function(x) {
burl = base.url
getSpatial(freeway, direction, quantity = x, abspm_start = abspm_start, abspm_end = abspm_end,
start_search.date.str = start, end_search.date.str = end,
curl, base.url, granularity = 'hour', data.folder)
})
)
}
build_spatial_multistation_df <- function(freeways_df, start, end, quantities, base.url) {rbindlist(lapply(data.frame(t(freeways)), function(x) {
name  <- x[[2]]
dir   <- x[[3]]
s_pm  <- x[[4]]
e_pm  <- x[[5]]
getSpatialMulti(name,dir, abspm_start = s_pm, abspm_end = e_pm, quantities, start_search.date.str = start, end_search.date.str = end,
curl = curl, base.url = base.url, data.folder = data.folder)
}))
}
getSpatialMulti("880", "S", '0','33', c('flow','occ'),start,end,curl,base.url,"hour",data.folder)
build_spatial_multistation_df <- function(freeways_df, start, end, quantities, base.url) {rbindlist(lapply(data.frame(t(freeways)), function(x) {
name  <- x[[2]]
dir   <- x[[3]]
s_pm  <- x[[4]]
e_pm  <- x[[5]]
getSpatialMulti(name,dir, abspm_start = s_pm, abspm_end = e_pm, quantities, start_search.date.str = start, end_search.date.str = end,
curl = curl, base.url = base.url, data.folder = data.folder)
}))
}
build_spatial_multistation_df(freeways, start, end, c("flow","occ","speed"), base.url = base.url)
write.csv(example_df, "../main/example.csv")
example_df <- build_spatial_multistation_df(freeways, start, end, c("flow","occ","speed"), base.url = base.url)
write.csv(example_df, "../main/example.csv")
dataframe_folder <- '/Users/sssantos/Documents/STA160/160trafficdata/df'
write.csv(example_df, paste(dataframe_folder, '/', 'example.csv', sep=''))
write.csv(example_df, paste(dataframe_folder, '/', 'example.csv', sep=''))
source('~/Documents/STA160/160trafficproject/get_data.R')
source('~/Documents/STA160/160trafficproject/get_data.R')
args()
args(example_df)
args(build_spatial_multistation_df)
freeways
# Using the main function to build the df
# NOTE: base.url always = base.url (issues when using as default)
# If you want to change freeways, do so from the freeways.csv
freeways[c(1,2),]
# Using the main function to build the df
# NOTE: base.url always = base.url (issues when using as default)
# To see the list of maze freeways, uncomment and run the following line
freeways
source('~/Documents/STA160/160trafficproject/get_data.R')
source('~/Documents/STA160/160trafficproject/get_data.R')
